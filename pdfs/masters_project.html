<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><script type="text/javascript" src="http://d3js.org/d3.v3.js" charset="utf-8"></script><script type="text/javascript" src="/javascripts/d3.tip.js"></script><script type="text/javascript" src="/javascripts/columnChart.js"></script><script type="text/javascript" src="http://numericjs.com/numeric/lib/numeric-1.2.6.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jstat/1.5.2/jstat.min.js"></script><script type="text/javascript" src="https://protobi.com/examples/pca/pca.js"></script><title>  · Aimee Barciauskas</title><meta name="description" content=" - Aimee Barciauskas"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600"></head><body><header><a href="/" class="logo-link"><img></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abarciauskas-bgse" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title"></h1><div class="post-time">Jun 29, 2016</div><div class="post-content"><p></p><h1 style="font-size: 42px">Algorritmo</h1><p></p>
<h3>Making Music with and Visualization of Recurrent Neural Networks</h3>

<p><em>Author:</em><br><br>Aimee Barciauskas<br><br>aimee.barciauskas@barcelonagse.eu</p>
<p><em>Supervisor:</em><br><br>Fernando Cucchietti<br><br>Scientific Visualization Group Leader, Barcelona Supercomputing Center (BSC)<br><br>fernando.cucchietti@bsc.es</p>
<p>2016 June 27</p>
<p>[toc]</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>This project is an exploration into neural networks, how to visualize them and how they may be used to produce music. It was done in collaboration with the Scientific Visualization Group of the Barcelona Supercomputing Center.</p>
<h1 id="Project-Overview"><a href="#Project-Overview" class="headerlink" title="Project Overview"></a>Project Overview</h1><p>Apart from the current work (this paper and it’s presentation), the project is presented at Sonar Music Festival as part of the Sonar+D creativity and technology marketplace. Visitors to Sonar+D participated in the music generation process by drawing and composing music using interactive applications and voting on sounds produced by these applications.</p>
<p>In addition to the interactive applications, it is important that the booth include an explanatory or educational component: Polling prior to the festival showed popular interest in learning about how artificial intelligence produces music. An integral part of the final presentation is a tool to visualize the network to visitors and educate them on how the new music had been created.</p>
<p>This project is comprised 2 parts:</p>
<ol>
<li>Can machines demonstrate creativity through machine learning algorithms (neural networks)?</li>
<li>How can we visualize neural networks?</li>
</ol>
<p>Though these parts are self-contained, in practice they are co-dependent. To visualize something one first has to understand it. Futher, the exercises of creating, viewing and interacting with a visualization support understanding.</p>
<h1 id="Part-1-Can-machines-produce-music"><a href="#Part-1-Can-machines-produce-music" class="headerlink" title="Part 1: Can machines produce music?"></a>Part 1: Can machines produce music?</h1><p>A popular field of research is whether machines can learn to be creative and the most popular algorithms being used to develop this research are neural networks.</p>
<h2 id="Using-recurrent-neural-networks-to-generate-music"><a href="#Using-recurrent-neural-networks-to-generate-music" class="headerlink" title="Using recurrent neural networks to generate music"></a>Using recurrent neural networks to generate music</h2><p>Recurrent neural networks are networks built using neurons that have a “memory”. During training in traditional feed-forward neural networks, neurons are trained on each input-output pair irrespective of sequence in the training data. Recurrent neural networks account for serially dependent data and as such have become popular in the applications of music generation and speech-to-text recognition and generation.</p>
<p>In practice, a special type of recurrent neuron called long short-term memory (LSTM) is used. Vanilla recurrent neurons often demonstrate vanishing or exploding gradients, and LSTM’s avoid this problem using multiple gates which “forget” inputs which are learned to likely be irrelevant.</p>
<p>To develop understanding of RNN’s, python code for each neuron type has been developed as part of this project.</p>
<h3 id="Development-of-single-neurons"><a href="#Development-of-single-neurons" class="headerlink" title="Development of single neurons"></a>Development of single neurons</h3><p>To have a deep understanding neural networks, it is fundamental to understand the single neuron and how it contributes to the the network. This is reminiscent to fundamentals of dynamic programming: by solving multiple sub-problems, you can arrive at the optimal solution to the larger problem.</p>
<p>At the final layer, the output of the neurons is passed into a loss function. The loss calculated at the output layer is then chained back through the network in order to determine how each set of weights to each neuron is impacting the loss. The weights are updated accordingly. Subproblems (local gradients) are calculated locally but updated according to their contribution globally (the loss or error)</p>
<h3 id="Code-for-single-neurons"><a href="#Code-for-single-neurons" class="headerlink" title="Code for single neurons"></a>Code for single neurons</h3><h4 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single neuron as a linear classifier</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.mlab <span class="keyword">as</span> mlab</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate X and Y</span></span><br><span class="line"><span class="comment"># XOR pattern</span></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights, a set for class 0 and a set for class 1</span></span><br><span class="line"><span class="comment"># W is dimension (dim of X x number of classes); an array of weights</span></span><br><span class="line"><span class="comment"># python does it cols x rows</span></span><br><span class="line"><span class="comment"># 1 - like a nnet with 1 neuron</span></span><br><span class="line">W = <span class="number">2</span>*np.random.random((<span class="number">3</span>,<span class="number">4</span>)) - <span class="number">1</span></span><br><span class="line">loss_all = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid activation function</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">    obs = X[i,:]</span><br><span class="line">    y_obs = Y[i]</span><br><span class="line">    losses = []</span><br><span class="line">    iters = <span class="number">1000</span></span><br><span class="line">    <span class="comment"># first neuron's weights</span></span><br><span class="line">    <span class="comment"># we have four sets for future times where we have more than one neuron</span></span><br><span class="line">    weights_neuron1 = W[:,<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(iters):</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'starting iter: '</span> + str(iter)</span><br><span class="line">        <span class="comment"># calculate the input to the first neuron</span></span><br><span class="line">        input_neuron1 = np.dot(weights_neuron1, obs) </span><br><span class="line">        <span class="comment"># calculate activation function</span></span><br><span class="line">        <span class="comment"># sigmoid in this case</span></span><br><span class="line">        f_neuron1 = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-input_neuron1))</span><br><span class="line">        <span class="comment"># this is the ouptput of the neuron</span></span><br><span class="line">        <span class="comment"># if y = 1, f_neuron is the correct probability</span></span><br><span class="line">        <span class="comment"># if y = 0, 1-f_neuron1 is the correct probability</span></span><br><span class="line">        true_class_prob = f_neuron1 <span class="keyword">if</span> y_obs == <span class="number">1</span> <span class="keyword">else</span> (<span class="number">1</span>-f_neuron1)</span><br><span class="line">        loss_neuron1 = <span class="number">1</span> - true_class_prob</span><br><span class="line">        losses.append(loss_neuron1)</span><br><span class="line">        dLdF = <span class="number">1</span> <span class="keyword">if</span> y_obs == <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        dL = loss_neuron1 * dLdF</span><br><span class="line">        <span class="comment"># the gradient for this neuron and this input</span></span><br><span class="line">        dW = dL * f_neuron1 * (<span class="number">1</span> - f_neuron1)</span><br><span class="line">        <span class="comment">#weights_neuron1 += np.dot(obs, dW)</span></span><br><span class="line">        weights_neuron1 += obs * dW</span><br><span class="line">    loss_all.append(losses)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = range(iters)</span><br><span class="line">line = plt.plot(x, loss_all[<span class="number">0</span>],  x, loss_all[<span class="number">1</span>], x, loss_all[<span class="number">2</span>], x, loss_all[<span class="number">3</span>], linewidth=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Users/aimeebarciauskas/Projects/bgse/second_term/data_visualization/source/images/neurons/figure_1.png" alt="losses"></p>
<h4 id="Vanilla-recurrent-neuron"><a href="#Vanilla-recurrent-neuron" class="headerlink" title="Vanilla recurrent neuron"></a>Vanilla recurrent neuron</h4><p><a href="https://github.com/abarciauskas-bgse/masters_project/blob/master/python_scripts/simple_rnn_beerstackoverflow.ipynb" target="_blank" rel="external">A simple RNN on beer.stackoverflow.com data</a></p>
<h4 id="LSTM-neuron"><a href="#LSTM-neuron" class="headerlink" title="LSTM neuron"></a>LSTM neuron</h4><p><a href="https://github.com/abarciauskas-bgse/masters_project/blob/master/lstm/my_simple_lstm.py.ipynb" target="_blank" rel="external">My Simple LSTM Neuron</a></p>
<h2 id="Generating-Music"><a href="#Generating-Music" class="headerlink" title="Generating Music"></a>Generating Music</h2><p>In one sense, music generation is both a classic application of recurrent neural networks: At each time step, the current step is used to predict the next step, and this is what RNN’s are built to do. However there are varied approaches in how to represent a timestep and discretize music. In fact, a deep understanding of digital music will improve future efforts.</p>
<h3 id="An-Experiment"><a href="#An-Experiment" class="headerlink" title="An Experiment"></a>An Experiment</h3><p>A network using the unique set of chords from the training set is used as a classification task for the neural network. For example, given the training set is comprised 381 unique chords, this is the set of possible classes in the classification task.</p>
<p>The upside of this approach is it presents a classic paradigm with which to represent the music generation task. The downside of this approach is it restricts the output space to the input space. This is resasonable if the goal is to generate music which sounds like the training set. However it misses the mark if the goal is a more creative generation process.</p>
<p><a href="https://github.com/abarciauskas-bgse/masters_project/blob/master/python_scripts/models1.ipynb" target="_blank" rel="external">Source code</a></p>
<p><strong>Technical Details</strong></p>
<ul>
<li><a href="https://github.com/fchollet/keras" target="_blank" rel="external">Keras library for 2-layer LSTM architecture</a></li>
<li><a href="ramhiser.com/2016/01/05/installing-tensorflow-on-an-aws-ec2-instance-with-gpu-support/">AWS GPU-backed AMI</a></li>
</ul>
<h3 id="Music-Generation-at-Sonar"><a href="#Music-Generation-at-Sonar" class="headerlink" title="Music Generation at Sonar"></a>Music Generation at Sonar</h3><p>Members of the Scientific Visualization group used a neural network composed of 6 LSTM layers of around 150 neurons per layer. The input to the network is a long vector representing the 5 tracks of 5 instruments, having some variation in space due to octave range. An additional dimension is included indicating whether the note is a note previously held.</p>
<p>Throughout Sonar+D, the network was used to create “generations” of songs. Each generation used a progressively better set as ranked by the audience at Sonar.</p>
<h1 id="Part-2-How-can-we-visualize-neural-networks"><a href="#Part-2-How-can-we-visualize-neural-networks" class="headerlink" title="Part 2: How can we visualize neural networks?"></a>Part 2: How can we visualize neural networks?</h1><h2 id="Overview-and-Motivation"><a href="#Overview-and-Motivation" class="headerlink" title="Overview and Motivation"></a>Overview and Motivation</h2><p>There are 2 objectives in visualizing neural networks: to educate and to understand.</p>
<p>Though neural networks have been around for a while, there is little understanding of why and how neural networks work. This has motivated some academic work in visualizing neural networks to understand what’s going on in the different components and levels of neural network architecture and improve performance.</p>
<p>Neural networks are being used in many every day applications and those interested should have an opportunity to understand how they work. The objectives are to provide transparency and satisfy curiousities.</p>
<p>It is the expectation that the audience for the visualization are people with no prior knowledge of computer science or machine learning. However, it was found the application as a standalone feature is probably insufficient material to educate such an audience (see <strong>User Experience Testing</strong>). For Sonar, this is acceptable as the author and colleagues were available to use the visualization as an educational aid and not a standalone resource.</p>
<h2 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h2><p>What follows is a review of current efforts in visualizing neural networks focused on both convolutional and recurrent neural networks.</p>
<p><strong><a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="external">Visualizing and Understanding Convolutional Networks, Zeiler and Fergus</a></strong></p>
<p>What concepts does a neural network learn to be important for image classification? How do concepts develop after intial layers? Visualization of a convolutional network using a deconvolutional network (or <a href="https://www.cs.nyu.edu/~gwtaylor/publications/zeilertaylorfergus_iccv2011.pdf" target="_blank" rel="external">deconvnet</a>) leverages how a deconvnet maps feature activities back to the original pixel input space. Deconvnets can be used to invert a trained neural network to glean learned concepts and visualize them. Details of the inversion are discussed in fine-grained detail in the paper.</p>
<p>Zeiler and Fergus produced visualizations using the popularized convnet architectures developed by <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" target="_blank" rel="external">LeCun et. al.</a> and <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="external">Krishevsky et. al.</a>. The visual analysis helped them beat the AlexNet 2012 single-model result by 1.7% using smaller strides (2 vs. 4) and smaller filters (7x7 vs. 11x11).</p>
<p><strong><a href="http://yosinski.com/media/papers/" target="_blank" rel="external">DeepVis, Jason Yosinski</a></strong></p>
<p>Jason Yosinski, DeepVis, 2015 published an open source tool, <a href="http://yosinski.com/deepvis" target="_blank" rel="external">DeepVis</a>, for visualizing a neural network in real time, with an image or camera feed. The tool is a dashboard of the network, enabling the user to visualize and interact with intermediary results of the network.</p>
<p>The strength of DeepVis may be in its simplicity. For example, it includes activation plots values which enable the user to see all the data.</p>
<blockquote>
<p>Although this visualization is simple to implement, we find it informative because all data flowing through the network can be visualized. There is nothing mysterious happening behind the scenes. (pg 4)</p>
</blockquote>
<p>Though the features of DeepVis are not innovative themselves, these visualizations yielded new and surprising intuitions:</p>
<ul>
<li>Layers demonstrate locality; that is layers become detectors of different real-world objects like flowers or faces. This suggests that intermediate layers become responsible for different concepts important in a well-trained classifer.</li>
<li>When an image does not include anything from the training set of classes, the real-time probability vector exposed a high sensitivity to small changes in input. In other words, shifting in your chair could mean instead of a cat you are classified as a lamp.</li>
<li>Although higher layers exhibit greater sensitivity, lower level computations are robust:</li>
</ul>
<blockquote>
<p>…the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision.</p>
</blockquote>
<p>Yosinski and Zeigler both expressed surprise at finding the features learned by intermediate layers are discernable and important to final classification. This is cool because it suggests the network learns the concepts like text or wheel before it becomes important in classifying books and cars. However, Yonsinski also comments:</p>
<blockquote>
<p>That said, not all features correspond to natural parts, raising the possibility of a different decomposition of the world than humans might expect. These visualizations suggest that further study into the exact nature of learned representations — whether they are local to a single channel or distributed across several — is likely to be interesting. (pg 9)</p>
</blockquote>
<p>The features of the tool and links on how to install it are available here: <a href="http://yosinski.com/deepvis" target="_blank" rel="external">DeepVis</a>.</p>
<p><strong><a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="external">Visualizing what ConvNets Learn, Andrej Karpathy</a></strong></p>
<p><a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="external">Visualizing What Convnets Learn</a> lists and describes useful ways to visualize a variety components of neural networks, including layer activations and visualizing high dimensional feature topologies.</p>
<p>Karpathy also has a cool interactive tool to demonstrate classification by a neural network in 2-dimensions: <a href="cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvnetJS demo</a>.</p>
<p><strong><a href="http://playground.tensorflow.org" target="_blank" rel="external">A Neural Network Playground, Daniel Smilkov and Shan Carter</a></strong></p>
<p>The <a href="http://playground.tensorflow.org" target="_blank" rel="external">Neural Network Playground</a> developed by Daniel Smilkov and Shan Carter offers a user-friendly and attractive interactive tool for understanding at a high level how neural networks work and how you might tune them.</p>
<p><strong><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank" rel="external">Neural Networks, Manifolds, and Topology, Chris Olah</a></strong></p>
<p>Colah and Karpathy both provide helpful animations of how space can be warped such that non-linear data is linearly seperable. One thing I like about Colah’s article in particular is he breaks this down into it’s component parts (weighting, translation and activation function) and provides a visualization of that process in fine-grained detail.</p>
<p>Colah’s intuitive explanations and visualizations of the challenge of warping space such that naturally non-linear data may be linearly seperated. I highly recommend a reading to gain an intuition through simple examples. Colah builds on the intuitions gleaned from this simplified examples to theorize about lower-bounds on the dimensionality requirements of neural networks (i.e. the Manifold Hypothesis).</p>
<p>Colah also has some very good articles on visualizing high-dimensional data, which is relevant but perhaps tangential to this topic:</p>
<ul>
<li><a href="http://colah.github.io/posts/2015-01-Visualizing-Representations/" target="_blank" rel="external">Visualizing Representations: Deep Learning and Human Beings</a></li>
<li><a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" target="_blank" rel="external">Visualizing MNIST: An Exploration of Dimensionality Reduction</a></li>
</ul>
<p><strong><a href="http://googleresearch.blogspot.com.es/2015/06/inceptionism-going-deeper-into-neural.html" target="_blank" rel="external">Inceptionism: Going Deeper into Neural Networks, Google</a></strong></p>
<p>Google also “inverts” a trained neural network classifier to compose images from noise, resulting in the well-known inceptionism of a trained neural network which can be used to create continuous surrealist abstractions of an image class. The methodology is, given you have a trained classifier and you want to determine what types of image patterns the classifier has learned, you start with an image of random noise and tweak the image until you get something the classifier finds to be a banana with high probability. This results in the surreal images now identified with Google’s inceptionism idea and help identifiy flaws in a training set.</p>
<p><strong><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTMs, Christopher Olah</a></strong></p>
<p>Olah uses simple diagrams and simplified computational graphs to explain Long Short-Term Memory Networks. He shows through sequential highlighting of a single LSTM neuron how it works like a conveyor belt with multiple gates.</p>
<p><strong><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy</a></strong></p>
<p>Karpathy’s blog post on RNN’s has been referenced in just about every other blog post about RNN’s I’ve come across (which now includes my own blog).</p>
<p>Karpathy uses super-simple diagrams to show how RNN’s can be “unrolled”. These super-simple diagrams reflect Karpathy’s view of neurons and layers as “building blocks” in neural networks, and thus can be used to demonstrante how inputs and outputs can have varied manifestations: one-to-one, many-to-one, one-to-many, many-to-many.</p>
<p>This post also includes a very cool visualization of how different neurons “fire” given some input: one can see how different neurons have responsibility for different features of the inputs.</p>
<h3 id="Literature-Review-Conclusions"><a href="#Literature-Review-Conclusions" class="headerlink" title="Literature Review: Conclusions"></a>Literature Review: Conclusions</h3><ul>
<li>Approaches by academics have been targeted at research interests, and while not directly instructive in creating experiences for a general public, they still offer insight into neural networks: For example, that intermediate layers also may be identifying and isolating important concepts for classification (and thus are more interpretable than originally thought).</li>
<li>There have been few attempts at interactivity and none at gamification as a way to tackle the education objective.</li>
<li>Many of the references included so far are in the domain of image classification, so it will be a thought adventure in how to generalize learnings and approaches into other domains and architectures.</li>
</ul>
<h2 id="Creative-Process"><a href="#Creative-Process" class="headerlink" title="Creative Process"></a>Creative Process</h2><p>An important part of the creative process is developing a low-level understanding of neurons and neural networks as described in Part 1 of this paper. The second part is sketching ideas on paper and discussing them with colleagues.</p>
<h3 id="Visualizing-a-Single-Neuron"><a href="#Visualizing-a-Single-Neuron" class="headerlink" title="Visualizing a Single Neuron"></a>Visualizing a Single Neuron</h3><p>A vital component in neural networks is back-propagation. The image below visualize back-propagation as a modification on the computational graph.</p>
<p><img src="single_neuron.tiff" alt="one-layer nnet"></p>
<p>The sketch for the final product:</p>
<p><img src="/Users/aimeebarciauskas/Projects/bgse/second_term/data_visualization/source/images/neurons/full.JPG" alt="full page view"></p>
<p>And screenshot:</p>
<p><img src="screenshot_neuron.png" alt="Neuron"></p>
<p>More on this process can be found on my blog: <a href="http://abarciauskas-bgse.github.io/2016/04/24/Visualizing-Neurons-for-a-Neural-Network/">Visualizing Neurons for a Neural Network</a></p>
<h3 id="Visualizing-Recurrent-Neural-Networks"><a href="#Visualizing-Recurrent-Neural-Networks" class="headerlink" title="Visualizing Recurrent Neural Networks"></a>Visualizing Recurrent Neural Networks</h3><p>To understand recurrent neural networks, the most useful tool is a computational graph:</p>
<p><img src="comp_graph_2lyrrnn.jpeg" alt="Computational Graph of a 2 layer RNN"></p>
<p>The sketch for the final product:</p>
<p><img src="rnn_sketch.JPG" alt="RNN Final"></p>
<p><img src="screenshot_rnn.png" alt="screenshot rnn final"></p>
<h2 id="User-Experience-Testing"><a href="#User-Experience-Testing" class="headerlink" title="User Experience Testing"></a>User Experience Testing</h2><p>Three user experience testing sessions informed various improvements to the project. The testing sessions were incredibly enlightening and humbling. Of course when working on a project, you are too close to it to possibly judge how other’s will experience it, and these sessions exposed the degree to which this is true.</p>
<p>In an interactive visualization the following lessons were learned:</p>
<ul>
<li>Users expect to be able to click on everything</li>
<li>All components should be labeled, either statically or using hovers</li>
<li>The content in these applications really requires a narrative and explanation. Walkthrus are included for guided learning.</li>
</ul>
<h2 id="Final-Product"><a href="#Final-Product" class="headerlink" title="Final Product"></a>Final Product</h2><p>The final visualization can be found at this<br><a href="https://calm-spire-21695.herokuapp.com/" target="_blank" rel="external">website</a>, and is showcased as part of the BSC booth at Sonar+D:</p>
<p><img src="sonar_booth.jpg" alt="Sonar Booth"><br><img src="sonar_booth_mayor.jpg" alt="Sonar Booth with Mayor"><br><img src="me_at_booth.jpg" alt="Me at Sonar Booth"></p>
<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1></div></article></div></section><footer><div class="paginator"></div><div class="copyright"><p>© 2015 - 2016 <a href="abarciauskas-bgse.github.io">Aimee Barciauskas</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>