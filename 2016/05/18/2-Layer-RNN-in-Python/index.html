<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><script type="text/javascript" src="http://d3js.org/d3.v3.js" charset="utf-8"></script><script type="text/javascript" src="/javascripts/d3.tip.js"></script><script type="text/javascript" src="/javascripts/columnChart.js"></script><script type="text/javascript" src="http://numericjs.com/numeric/lib/numeric-1.2.6.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jstat/1.5.2/jstat.min.js"></script><script type="text/javascript" src="https://protobi.com/examples/pca/pca.js"></script><title> 2-Layer Recurrent Neural Network (Python) · Aimee Barciauskas</title><meta name="description" content="2-Layer Recurrent Neural Network (Python) - Aimee Barciauskas"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600"></head><body><header><a href="/" class="logo-link"><img></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abarciauskas-bgse" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">2-Layer Recurrent Neural Network (Python)</h1><div class="post-time">May 18, 2016</div><div class="post-content"><p><em>This post is part of ongoing work on my master’s project in visualizing neural networks for a general public.</em></p>
<p>The objective of this post is to share my experience and source code for a 2-layer recurrent neural network.</p>
<p>I’m not the first and I probably won’t be the last to refer to Andrej Karpathy’s blog on <strong>Recurrent Neural Networks</strong>: <a href="karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. In that post, he refers to <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="external"><code>min-char-rnn.py</code></a>, a 100-line gist for a vanilla RNN with a single-layer.</p>
<p>My code builds on Andrej’s code to produce an RNN with a second layer. This exercise confirmed the importance of detailing the computational graph of a network. Computational graphs are a tool introduced to me by Karpathy’s cs231 course on convolutional neural networks. I sketched many versions - many versions simply ran out of horizontal space to draw the network. Here is the final version:</p>
<p><img src="/images/comp_graph_2lyrrnn.jpeg" alt="Computational Graph: 2-layer RNN"></p>
<p><em>Note: Code refers to <code>dh0next</code> and <code>dhnext</code> which in the image above are <code>dh0t-1</code> and <code>dh1t-1</code></em></p>
<p>You can see this code in action using the dataset dujour (beer.stackoverflow posts) and compare results with a simple 1-layer rnn (spoiler alert: nothing significant). Code for the network itself is copy / pasted below.</p>
<p>Before you go, I find it important to repeat the common advice at this point is to start using <strong>LSTM</strong> (Long Short-Term Memory) networks. I have not yet born witness to their power, but am hopeful given my sampled output is still (mostly) gibberish. Every once in a while I see “beer” and “IPAs” but there is nothing intelligible beyond that. It’s possible I need to train for longer, still looking at heuristics, tuning and convergence rates to see what can be improved.</p>
<p>To learn more about LSTM’s, I strongly recommend a post by Chris Olah: <a href="colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>.</p>
<h2 id="Code-for-2-layer-RNN"><a href="#Code-for-2-layer-RNN" class="headerlink" title="Code for 2-layer RNN"></a>Code for 2-layer RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span><span class="params">(inputs, targets, hprev_l0, hprev_l1)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(inputs) == len(targets)</span><br><span class="line">    T = len(inputs)</span><br><span class="line">    x = &#123;&#125;</span><br><span class="line">    h0 = &#123;&#125;</span><br><span class="line">    h1 = &#123;&#125;</span><br><span class="line">    <span class="comment"># the last hidden state is given</span></span><br><span class="line">    h0[-<span class="number">1</span>] = np.copy(hprev_l0)</span><br><span class="line">    h1[-<span class="number">1</span>] = np.copy(hprev_l1)</span><br><span class="line">    l0 = &#123;&#125;</span><br><span class="line">    y = &#123;&#125;</span><br><span class="line">    p_state = &#123;&#125;</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        <span class="comment"># structure input</span></span><br><span class="line">        input_letter_idx = char_to_ix[inputs[t]]</span><br><span class="line">        x[t] = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">        x[t][input_letter_idx] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># FORWARD PASS </span></span><br><span class="line">        <span class="comment"># Layer 0</span></span><br><span class="line">        h0[t] = np.tanh(np.dot(Wxh0, x[t]) + np.dot(Wh0h0, h0[(t-<span class="number">1</span>)]) + bh0)</span><br><span class="line">        l0[t] = np.dot(Wh0l0, h0[t]) + bl0</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 1</span></span><br><span class="line">        <span class="comment"># l0[t] is the input to the second layer</span></span><br><span class="line">        h1[t] = np.tanh(np.dot(Wl0h1, l0[t]) + np.dot(Wh1h1, h1[(t-<span class="number">1</span>)]) + bh1)</span><br><span class="line">        y[t]  = np.dot(Wh1y, h1[t]) + by</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output layer</span></span><br><span class="line">        p_state[t] = np.exp(y[t])/np.sum(np.exp(y[t]))</span><br><span class="line">        target_letter_idx = char_to_ix[targets[t]]</span><br><span class="line">        loss += -np.log(p_state[t][target_letter_idx, <span class="number">0</span>])</span><br><span class="line">    dWxh0, dWh0h0, dbh0, dWh0l0, dbl0, dWl0h1, dWh1h1, dbh1, dWh1y, dby = np.zeros_like(Wxh0), \</span><br><span class="line">        np.zeros_like(Wh0h0), \</span><br><span class="line">        np.zeros_like(bh0), \</span><br><span class="line">        np.zeros_like(Wh0l0), \</span><br><span class="line">        np.zeros_like(bl0), \</span><br><span class="line">        np.zeros_like(Wl0h1), \</span><br><span class="line">        np.zeros_like(Wh1h1), \</span><br><span class="line">        np.zeros_like(bh1), \</span><br><span class="line">        np.zeros_like(Wh1y), \</span><br><span class="line">        np.zeros_like(by)</span><br><span class="line">    dh0next = np.zeros_like(h0[<span class="number">0</span>])</span><br><span class="line">    dh1next = np.zeros_like(h1[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        <span class="comment"># Backprop loss into y (e.g. loss from normalized log probabilities of y)</span></span><br><span class="line">        dy = np.copy(p_state[t])</span><br><span class="line">        target_letter_idx = char_to_ix[targets[t]]</span><br><span class="line">        dy[target_letter_idx] -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backprop through layer 1</span></span><br><span class="line">        dh1 = np.dot(Wh1y.T, dy) + dh1next</span><br><span class="line">        dWh1y += np.dot(dy, h1[t].T)</span><br><span class="line">        dby += dy</span><br><span class="line">        <span class="comment"># An update to layer 1's hidden state through non-linearity</span></span><br><span class="line">        dh1raw = (<span class="number">1</span> - h1[t]*h1[t]) * dh1</span><br><span class="line">        dbh1 += dh1raw</span><br><span class="line">        dWl0h1 += np.dot(dh1raw, l0[t].T)</span><br><span class="line">        dWh1h1 += np.dot(dh1raw, h1[t-<span class="number">1</span>].T)</span><br><span class="line">        dh1next = np.dot(Wh1h1.T, dh1raw)</span><br><span class="line"></span><br><span class="line">        dl0 = np.dot(Wl0h1.T, dh1raw) <span class="comment"># not sure if this should be dh1raw or dh1next</span></span><br><span class="line">        <span class="comment"># Backprop through layer 0 - Key difference: dy is replaced with dl0, l0 to hidden l1</span></span><br><span class="line">        dh0 = np.dot(Wh0l0.T, dl0) + dh0next</span><br><span class="line">        dWh0l0 += np.dot(dl0, h0[t].T)</span><br><span class="line">        dbl0 += dl0</span><br><span class="line">        dh0raw = (<span class="number">1</span> - h0[t]*h0[t]) * dh0</span><br><span class="line">        dbh0 += dh0raw</span><br><span class="line">        dWxh0 += np.dot(dh0raw, x[t].T)</span><br><span class="line">        dWh0h0 += np.dot(dh0raw, h0[t-<span class="number">1</span>].T)</span><br><span class="line">        dh0next = np.dot(Wh0h0.T, dh0raw)</span><br><span class="line">    <span class="keyword">for</span> dparam <span class="keyword">in</span> [dWxh0, dWh0h0, dbh0, dWh0l0, dbl0, dWl0h1, dWh1h1, dbh1, dWh1y, dby]:</span><br><span class="line">        np.clip(dparam, -<span class="number">5</span>, <span class="number">5</span>, out=dparam) <span class="comment"># clip to mitigate exploding gradients        </span></span><br><span class="line">    <span class="keyword">return</span> loss, \</span><br><span class="line">        dWxh0, dWh0h0, dbh0, dWh0l0, dbl0, dWl0h1, dWh1h1, dbh1, dWh1y, dby, \</span><br><span class="line">        h0[len(inputs)-<span class="number">1</span>], h1[len(inputs)-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-1</span></span><br><span class="line"><span class="comment"># number of neurons per layer (for now just one layer)</span></span><br><span class="line">hidden_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># model parameters (x - input, l0 - layer 0 output, y - layer 1 output)</span></span><br><span class="line">    <span class="comment"># Layer 0</span></span><br><span class="line">    <span class="keyword">global</span> Wxh0</span><br><span class="line">    Wxh0  = np.random.randn(hidden_size, vocab_size)*<span class="number">0.01</span> <span class="comment"># input x to hidden l0</span></span><br><span class="line">    <span class="keyword">global</span> Wh0h0</span><br><span class="line">    Wh0h0 = np.random.randn(hidden_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden l0 to hidden l0</span></span><br><span class="line">    <span class="keyword">global</span> bh0</span><br><span class="line">    bh0   = np.zeros((hidden_size, <span class="number">1</span>)) <span class="comment"># hidden bias l0</span></span><br><span class="line">    <span class="keyword">global</span> Wh0l0</span><br><span class="line">    Wh0l0 = np.random.randn(vocab_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden l0 to l0</span></span><br><span class="line">    <span class="keyword">global</span> bl0</span><br><span class="line">    bl0   = np.zeros((vocab_size, <span class="number">1</span>)) <span class="comment"># l0 output bias</span></span><br><span class="line">    <span class="comment"># Layer 1 - input is l0</span></span><br><span class="line">    <span class="keyword">global</span> Wl0h1</span><br><span class="line">    Wl0h1 = np.random.randn(hidden_size, vocab_size)*<span class="number">0.01</span> <span class="comment"># input to hidden l1</span></span><br><span class="line">    <span class="keyword">global</span> Wh1h1</span><br><span class="line">    Wh1h1 = np.random.randn(hidden_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden l1 to hidden l1</span></span><br><span class="line">    <span class="keyword">global</span> bh1</span><br><span class="line">    bh1   = np.zeros((hidden_size, <span class="number">1</span>)) <span class="comment"># hidden bias l1</span></span><br><span class="line">    <span class="keyword">global</span> Wh1y</span><br><span class="line">    Wh1y  = np.random.randn(vocab_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden l1 to output y</span></span><br><span class="line">    <span class="keyword">global</span> by</span><br><span class="line">    by    = np.zeros((vocab_size, <span class="number">1</span>)) <span class="comment"># output bias</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(data, vocab_size, max_iters, batch_size = <span class="number">25</span>, sample = False)</span>:</span></span><br><span class="line">    reset_params()</span><br><span class="line">    losses = []</span><br><span class="line">    <span class="comment"># Have some memory of Weights for adagrad:</span></span><br><span class="line">    mWxh0, mWh0h0, mbh0, mWh0l0, mbl0, mWl0h1, mWh1h1, mbh1, mWh1y, mby = \</span><br><span class="line">      np.zeros_like(Wxh0), np.zeros_like(Wh0h0), np.zeros_like(bh0), np.zeros_like(Wh0l0), \</span><br><span class="line">      np.zeros_like(bl0), np.zeros_like(Wl0h1), np.zeros_like(Wh1h1), np.zeros_like(bh1), np.zeros_like(Wh1y), np.zeros_like(by)</span><br><span class="line">    pointer = <span class="number">0</span></span><br><span class="line">    hprev_l0 = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">    hprev_l1 = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">    smooth_loss = -np.log(<span class="number">1.0</span>/vocab_size)*batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):    </span><br><span class="line">        <span class="keyword">if</span> pointer + batch_size + <span class="number">1</span> &gt; len(data):</span><br><span class="line">            pointer = <span class="number">0</span></span><br><span class="line">            hprev_l0 = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">            hprev_l1 = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">        inputs = data[pointer:(batch_size + pointer)]</span><br><span class="line">        targets = data[(pointer + <span class="number">1</span>):(batch_size + pointer + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sample from the model now and then</span></span><br><span class="line">        <span class="keyword">if</span> sample <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> i % round(max_iters/<span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">            lidx = char_to_ix[<span class="string">'t'</span>]</span><br><span class="line">            sample_ix = sample(hprev_l0, hprev_l1, lidx, <span class="number">200</span>)</span><br><span class="line">            txt = <span class="string">''</span>.join(ix_to_char[ix] <span class="keyword">for</span> ix <span class="keyword">in</span> sample_ix)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'----\n %s \n----'</span> % (txt, )</span><br><span class="line"></span><br><span class="line">        loss, \</span><br><span class="line">            dWxh0, dWh0h0, dbh0, dWh0l0, dbl0, dWl0h1, dWh1h1, dbh1, dWh1y, dby, \</span><br><span class="line">            hprev_l0, hprev_l1 = loss_function(inputs, targets, hprev_l0, hprev_l1)</span><br><span class="line">        <span class="comment"># do adagrad update on parameters</span></span><br><span class="line">        <span class="comment"># perform parameter update with Adagrad</span></span><br><span class="line">        <span class="keyword">for</span> param, dparam, mem <span class="keyword">in</span> zip([Wxh0,  Wh0h0,   bh0,  Wh0l0,  bl0,  Wl0h1,  Wh1h1,  bh1,  Wh1y, by], </span><br><span class="line">                                      [dWxh0, dWh0h0, dbh0, dWh0l0, dbl0, dWl0h1, dWh1h1, dbh1, dWh1y, dby], </span><br><span class="line">                                      [mWxh0, mWh0h0, mbh0, mWh0l0, mbl0, mWl0h1, mWh1h1, mbh1, mWh1y, mby]):</span><br><span class="line">            mem += dparam * dparam</span><br><span class="line">            param += -learning_rate * dparam / np.sqrt(mem + <span class="number">1e-8</span>) <span class="comment"># adagrad update          </span></span><br><span class="line">        smooth_loss = smooth_loss * <span class="number">0.999</span> + loss * <span class="number">0.001</span></span><br><span class="line">        losses.append(smooth_loss)</span><br><span class="line">        pointer += batch_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'losses'</span>: losses, <span class="string">'h0'</span>: hprev_l0, <span class="string">'h1'</span>: hprev_l1&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(h0prev, h1prev, seed_idx, nsamples)</span>:</span></span><br><span class="line">    sampled_idcs = []</span><br><span class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">    x[seed_idx] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nsamples):</span><br><span class="line">        h0 = np.tanh(np.dot(Wxh0, x) + np.dot(Wh0h0, h0prev) + bh0)</span><br><span class="line">        l0 = np.dot(Wh0l0, h0) + bl0</span><br><span class="line">        h1 = np.tanh(np.dot(Wl0h1, l0) + np.dot(Wh1h1, h1prev) + bh1)</span><br><span class="line">        y = np.dot(Wh1y, h1) + by</span><br><span class="line">        p = np.exp(y)/np.sum(np.exp(y))</span><br><span class="line">        ix = np.random.choice(range(vocab_size), p = p.ravel())</span><br><span class="line">        x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">        x[ix] = <span class="number">1</span></span><br><span class="line">        sampled_idcs.append(ix)</span><br><span class="line">    <span class="keyword">return</span> sampled_idcs</span><br></pre></td></tr></table></figure>
</div></article></div></section><footer><div class="paginator"><a href="/2016/05/10/Topic-modeling-posts-on-beer-stackoverflow-com/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2016 <a href="abarciauskas-bgse.github.io">Aimee Barciauskas</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>