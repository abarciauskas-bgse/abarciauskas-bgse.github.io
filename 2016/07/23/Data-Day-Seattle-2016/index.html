<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><script type="text/javascript" src="http://d3js.org/d3.v3.js" charset="utf-8"></script><script type="text/javascript" src="/javascripts/d3.tip.js"></script><script type="text/javascript" src="/javascripts/columnChart.js"></script><script type="text/javascript" src="http://numericjs.com/numeric/lib/numeric-1.2.6.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jstat/1.5.2/jstat.min.js"></script><script type="text/javascript" src="https://protobi.com/examples/pca/pca.js"></script><title> Data Day Seattle 2016 · Aimee Barciauskas</title><meta name="description" content="Data Day Seattle 2016 - Aimee Barciauskas"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600"></head><body><header><a href="/" class="logo-link"><img></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abarciauskas-bgse" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Data Day Seattle 2016</h1><div class="post-time">Jul 23, 2016</div><div class="post-content"><p>I was talking to a few other attendees of <a href="http://2016.datadayseattle.com/" target="_blank" rel="external">Data Day Seattle</a>, Hannah and Stephanie, and realized what made this conference most especially cool to me, personally: it feels as if I and Seattle, the town where I grew up, parted ways (amicably), ten years ago, and now we’ve reunited to find we’re perfect for each other. But maybe a little too perfect. Like it’s a little creepy and we need to plan some time apart so we don’t start to look like each other.</p>
<p>Anyways, I have never had more difficulty chosing sessions at a conference than when attending the small-ish (that is, perfectly sized) <a href="http://2016.datadayseattle.com/schedule" target="_blank" rel="external">Data Day Seattle</a>.</p>
<p>But since I have not been optimized for concurrency, ultimately I attended the following sessions:</p>
<h2 id="Make-Better-Technology-Decisions-Plenary-Keynote"><a href="#Make-Better-Technology-Decisions-Plenary-Keynote" class="headerlink" title="Make Better Technology Decisions, Plenary Keynote"></a>Make Better Technology Decisions, Plenary Keynote</h2><p><strong>Speaker: Charity Majors | @mipsytipsy</strong></p>
<blockquote>
<p>Software is eating the world (- Andreesen Horowitz)</p>
<p>…and your brain probably (- Charity)</p>
</blockquote>
<p>Since software will soon be coursing through our veins and brains, let’s make better decisions with that in mind.</p>
<p>The problem (perhaps especially true in contemporary times and beyond) is the paradox of choice in technologies. This is particularly hard on humans. </p>
<p><img src="/images/welcome-to-the-jungle.png" alt="Welcome to the Jungle"></p>
<p>Charity listed 6 tips for navigating the jungle, below I list my 3 favorites:</p>
<ol>
<li>Use technology to serve the mission. Building software is not your mission.</li>
<li>Choose boring technology when you can. Limit exposure to unknown unknowns.</li>
<li>Spend your risk tokens on key differentiations.</li>
</ol>
<h2 id="How-Machine-Learning-is-Like-Cycling"><a href="#How-Machine-Learning-is-Like-Cycling" class="headerlink" title="How Machine Learning is Like Cycling"></a>How Machine Learning is Like Cycling</h2><p><strong>Speaker: Michelle Casbon | @texasmichelle</strong></p>
<p>Michelle’s talk was inspirational to the timid or unexperienced among us. Michelle felt unsure of herself joining a natural language processing (NLP) startup with NLP experts and PhDs. But she worked hard, learned from both her own and others’ mistakes, and didn’t permit herself artificial boundraies.</p>
<p>These statements are true for both her experience in cycling and applying machine learning in NLP. Michelle cycled from San Francisco to Las Vegas on a whim (it sounded like there was some peer pressure involved) and now heads Data Science at Qordoba.</p>
<h2 id="word2vec-LDA-and-introducing-a-new-hybrid-algorithm-lda2vec"><a href="#word2vec-LDA-and-introducing-a-new-hybrid-algorithm-lda2vec" class="headerlink" title="word2vec, LDA, and introducing a new hybrid algorithm: lda2vec"></a>word2vec, LDA, and introducing a new hybrid algorithm: lda2vec</h2><p><strong>Christopher Moody | @cemoody</strong></p>
<p>I’m pretty excited about allthethings! natural language processing and text mining so Moody’s explanations and experimentations were super fun.</p>
<p>I’m a noob to word2vec, but I plan to learn more and try it out as soon as possible. Moody summarized word2vec with the following example:</p>
<blockquote>
<p>king - man + women = queen</p>
</blockquote>
<p>The above is the representation operations on vectors produced by a <a href="http://deeplearning4j.org/word2vec" target="_blank" rel="external">word2vec neural network</a>.</p>
<p>Moody described how word2vec vectors are dense representations of words via features extracted from a given corpus. This is valuable because it gives us a rich representation of a word, but is hard to interpret.</p>
<p>Another cool thing about word2vec is you can derive the directions which represent ideas, such as the “feminine” direction:</p>
<p><img src="/images/femine_direction.jpeg" alt="femine_direction"></p>
<p><a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" target="_blank" rel="external">Latent Dirichlet Allocation (LDA)</a> produces sparse vectors of topics as distributions over words, given a corpus. These vectors are highly interpretable, given their representation as probabilities over words and level of sparsity.</p>
<p>Moody designed lda2vec using both methodologies <a href="https://github.com/cemoody/lda2vec" target="_blank" rel="external">lda2vec</a>. It’s complicated so I’m just going to leave these links here: find out more! on the <a href="http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec" target="_blank" rel="external">multithreaded blog</a> or the <a href="https://arxiv.org/abs/1605.02019" target="_blank" rel="external">lda2vec paper</a>.</p>
<h2 id="Graphs-vs-Tables-Ready-Fight"><a href="#Graphs-vs-Tables-Ready-Fight" class="headerlink" title="Graphs vs Tables: Ready? Fight."></a>Graphs vs Tables: Ready? Fight.</h2><p><strong>Dr. Denise Koessler Gosnell | @DeniseKGosnell</strong></p>
<p>Gosnell discussed her experiences using graph and relational databases to perform <a href="https://en.wikipedia.org/wiki/Onlineanalyticalprocessing" target="_blank" rel="external">Online Analytical Processing (OLAP)</a> queries on health care data sets. The objective sounded to me to be inferring relationships between providers, claims and insurance companies (e.g. deriving the “network” of an insurance plan).</p>
<p>In graphs, this included deriving queries to infer <a href="https://en.wikipedia.org/wiki/Triadicclosure" target="_blank" rel="external">triadic closures</a> between vertices which are not connected explicitly through the data. For example, trying to derive relationships between subscribers and providers via their explicit edges to transactions vertices.</p>
<p>As an interesting sidebar, another potential data storage paradigm for modeling this data suggested by an audience member is <a href="https://en.wikipedia.org/wiki/Triplestore" target="_blank" rel="external">RDFs (or triplestores)</a>.</p>
<p>In relational databases, the solution is to flatten the relations nested in structured json representations of the data into relational database columns. Turns out, these tables are just another flavor of an adjacency or bag-of-words matrices. So, it’s sparse and you can do all the typical text mining modelling stuff with it. Cool!</p>
<p>The punch line is: relational databases support adjacency matrices. And if you want similarity models you want matrices. But these are just adjacency matrices so it’s just another representation of the graph! And if you want interpretable (and cool) visualizations, you want graphs (and possibly graph databases).</p>
<p>So the solution to your problem is: it depends. Perhaps it’s a polyglot. Surprise!</p>
<h2 id="Finding-Key-Influencers-and-Viral-Topics-in-Twitter-Networks-Related-to-ISIS-and-to-the-2016-Primary-Elections"><a href="#Finding-Key-Influencers-and-Viral-Topics-in-Twitter-Networks-Related-to-ISIS-and-to-the-2016-Primary-Elections" class="headerlink" title="Finding Key Influencers and Viral Topics in Twitter Networks Related to ISIS and to the 2016 Primary Elections"></a>Finding Key Influencers and Viral Topics in Twitter Networks Related to ISIS and to the 2016 Primary Elections</h2><p><strong>Steve Kramer | @ParagonSci_Inc</strong></p>
<p>Another thing that gets me excited is graph theory and graph algorithms. And in Kramer’s talk they are being applied to text in cool ways so the excitement is compounded! Kramer introduced to me <a href="http://bit.ly/2a9h0BB" target="_blank" rel="external">k-cores</a> of graphs. This is another property and algorithm I hope to understand better and implement in the near future.</p>
<p>Kramer also mentioned a really awesome NLP tool I didn’t know about: <a href="http://liwc.wpengine.com/" target="_blank" rel="external">Linguistic Inquiry and Word Count (LIWC)</a>. LIWC is a library of advanced sentiment analysis dimensions. In other words instead of defining sentiment as the binary positive and negative, words are defined a wider spectrum of “thoughts, feelings, personality and movitivations”.</p>
<h1 id="Thank-You"><a href="#Thank-You" class="headerlink" title="Thank You!"></a>Thank You!</h1><p>A final thank you to all the <a href="http://2016.datadayseattle.com/speakers" target="_blank" rel="external">speakers</a>, <a href="http://datadayseattle.com/sponsors" target="_blank" rel="external">sponsors</a> and <a href="https://www.linkedin.com/groups/4233300/profile" target="_blank" rel="external">organizers</a>! It was truly one of the best concentrations of content of any conference I have attended.</p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/06/29/Master-s-Project-Visualizing-Neural-Networks/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2016 <a href="abarciauskas-bgse.github.io">Aimee Barciauskas</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>