<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><script type="text/javascript" src="http://d3js.org/d3.v3.js" charset="utf-8"></script><script type="text/javascript" src="/javascripts/d3.tip.js"></script><script type="text/javascript" src="/javascripts/columnChart.js"></script><script type="text/javascript" src="http://numericjs.com/numeric/lib/numeric-1.2.6.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jstat/1.5.2/jstat.min.js"></script><script type="text/javascript" src="https://protobi.com/examples/pca/pca.js"></script><title> Visualizing Neurons for a Neural Network · Aimee Barciauskas</title><meta name="description" content="Visualizing Neurons for a Neural Network - Aimee Barciauskas"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600"></head><body><header><a href="/" class="logo-link"><img></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abarciauskas-bgse" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Visualizing Neurons for a Neural Network</h1><div class="post-time">Apr 24, 2016</div><div class="post-content"><style>
  .post-content img {
    max-width: 60%;
  }
</style>



<p><em>This post is part of ongoing work on my master’s project in visualizing neural networks for a general public.</em></p>
<p>What are neurons for a neural network?</p>
<p>To understand, we start with a <a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">toy example</a>:</p>
<p><img src="/images/neurons/data.png" alt="data"></p>
<p><strong>Our objective is to correctly classify this data using linear planes.</strong></p>
<p>We can reveal that this is challenging with this particular data set with the following plot:</p>
<p><img src="/images/neurons/data_plot.png" alt="with plot"></p>
<p><img src="/images/neurons/plane.JPG" alt="with plane"></p>
<p>In addition to the given input (axon), we will denote <strong><code>x</code></strong>, A neuron in a neural network is comprised of the following components:</p>
<ul>
<li><strong><code>w</code>:</strong> the weights or synapses (initially chosen) for each input to the neuron</li>
<li><strong><code>f</code>:</strong> an activation function which is a function of <code>w</code> and <code>x</code>.</li>
<li><strong><code>o</code>:</strong> the output of the neuron (i.e. output axon) which is sent on to one or more other neurons as an input axon.</li>
</ul>
<p>It is helpful to understanding neurons by understanding their place in the larger picture of the network: The output axons of neurons in the final layer (or the first layer in our simplistic example) are passed a loss function. The loss calculated is then chained back through the network in order to determine how each set of weights to each neuron is impacting the loss. The weights are updated accordingly.</p>
<p>This process is represented in the following gif:</p>
<p><img src="/images/output_xCrmbs.gif" alt="one-layer nnet"></p>
<p>Or the coded version:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single neuron as a linear classifier</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.mlab <span class="keyword">as</span> mlab</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate X and Y</span></span><br><span class="line"><span class="comment"># XOR pattern</span></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights, a set for class 0 and a set for class 1</span></span><br><span class="line"><span class="comment"># W is dimension (dim of X x number of classes); an array of weights</span></span><br><span class="line"><span class="comment"># python does it cols x rows</span></span><br><span class="line"><span class="comment"># 1 - like a nnet with 1 neuron</span></span><br><span class="line">W = <span class="number">2</span>*np.random.random((<span class="number">3</span>,<span class="number">4</span>)) - <span class="number">1</span></span><br><span class="line">loss_all = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid activation function</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">    obs = X[i,:]</span><br><span class="line">    y_obs = Y[i]</span><br><span class="line">    losses = []</span><br><span class="line">    iters = <span class="number">1000</span></span><br><span class="line">    <span class="comment"># first neuron's weights</span></span><br><span class="line">    <span class="comment"># we have four sets for future times where we have more than one neuron</span></span><br><span class="line">    weights_neuron1 = W[:,<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(iters):</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'starting iter: '</span> + str(iter)</span><br><span class="line">        <span class="comment"># calculate the input to the first neuron</span></span><br><span class="line">        input_neuron1 = np.dot(weights_neuron1, obs) </span><br><span class="line">        <span class="comment"># calculate activation function</span></span><br><span class="line">        <span class="comment"># sigmoid in this case</span></span><br><span class="line">        f_neuron1 = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-input_neuron1))</span><br><span class="line">        <span class="comment"># this is the ouptput of the neuron</span></span><br><span class="line">        <span class="comment"># if y = 1, f_neuron is the correct probability</span></span><br><span class="line">        <span class="comment"># if y = 0, 1-f_neuron1 is the correct probability</span></span><br><span class="line">        true_class_prob = f_neuron1 <span class="keyword">if</span> y_obs == <span class="number">1</span> <span class="keyword">else</span> (<span class="number">1</span>-f_neuron1)</span><br><span class="line">        loss_neuron1 = <span class="number">1</span> - true_class_prob</span><br><span class="line">        losses.append(loss_neuron1)</span><br><span class="line">        dLdF = <span class="number">1</span> <span class="keyword">if</span> y_obs == <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        dL = loss_neuron1 * dLdF</span><br><span class="line">        <span class="comment"># the gradient for this neuron and this input</span></span><br><span class="line">        dW = dL * f_neuron1 * (<span class="number">1</span> - f_neuron1)</span><br><span class="line">        <span class="comment">#weights_neuron1 += np.dot(obs, dW)</span></span><br><span class="line">        weights_neuron1 += obs * dW</span><br><span class="line">    loss_all.append(losses)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = range(iters)</span><br><span class="line">line = plt.plot(x, loss_all[<span class="number">0</span>],  x, loss_all[<span class="number">1</span>], x, loss_all[<span class="number">2</span>], x, loss_all[<span class="number">3</span>], linewidth=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>This works because we are only trying to classify one data point at a time.<br><img src="/images/neurons/figure_1.png" alt="losses"></p>
<h2 id="Other-ideas-for-visualization"><a href="#Other-ideas-for-visualization" class="headerlink" title="Other ideas for visualization"></a>Other ideas for visualization</h2><p><img src="/images/output_laxktO.gif" alt="simple neuron giff"><br><img src="/images/neurons/output_iH6chi.gif" alt="another animation"><br><img src="/images/neurons/output_5XLW65.gif" alt="another animation"><br><img src="/images/neurons/full.JPG" alt="full page view"></p>
<p><em>inspired by sound waves and circuits:</em></p>
<p><img src="/images/neurons/waves1.png" alt="waves1"><br><img src="/images/neurons/waves2.png" alt="waves2"><br><img src="/images/neurons/circuit.png" alt="circuit"><br><img src="/images/neurons/waves3.png" alt="waves3"></p>
<h3 id="Inspirations"><a href="#Inspirations" class="headerlink" title="Inspirations"></a>Inspirations</h3><p><a href="https://www.jasondavies.com/lloyd/" target="_blank" rel="external">https://www.jasondavies.com/lloyd/</a></p>
<p><a href="https://www.youtube.com/watch?v=t8g-iYGHpEA" target="_blank" rel="external">https://www.youtube.com/watch?v=t8g-iYGHpEA</a></p>
<p><a href="https://www.youtube.com/watch?v=kPRA0W1kECg&amp;ebc=ANyPxKoLSFpC4J1Ueq0R-X9jstKft6GHKRxwGltmJN3zXpHPaIPmsQ-sLB-Bo-0d2-ThGumIgaEbipmca4xLPG3r6wV-6PY76w" target="_blank" rel="external">https://www.youtube.com/watch?v=kPRA0W1kECg&amp;ebc=ANyPxKoLSFpC4J1Ueq0R-X9jstKft6GHKRxwGltmJN3zXpHPaIPmsQ-sLB-Bo-0d2-ThGumIgaEbipmca4xLPG3r6wV-6PY76w</a></p>
<p><a href="http://daftpunk.themaninblue.com/" target="_blank" rel="external">http://daftpunk.themaninblue.com/</a></p>
<p><a href="http://guns.periscopic.com/?year=2013" target="_blank" rel="external">http://guns.periscopic.com/?year=2013</a></p>
<p><a href="https://www.youtube.com/watch?v=4M0soXr2cHA" target="_blank" rel="external">https://www.youtube.com/watch?v=4M0soXr2cHA</a></p>
<p><a href="http://www.facebookstories.com/stories/2200/data-visualization-photo-sharing-explosions" target="_blank" rel="external">http://www.facebookstories.com/stories/2200/data-visualization-photo-sharing-explosions</a></p>
</div></article></div></section><footer><div class="paginator"><a href="/2016/04/18/Visualizing-kNN-with-MNIST-Final-DataViz-Project/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2016 <a href="abarciauskas-bgse.github.io">Aimee Barciauskas</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>